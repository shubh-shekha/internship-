{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2c9b3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\hp\\anaconda3\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\hp\\anaconda3\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests) (2023.7.22)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4 pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f92778e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 Name Rating  Year\n",
      "0                     Ship of Theseus      8  2012\n",
      "1                              Iruvar    8.4  1997\n",
      "2                     Kaagaz Ke Phool    7.8  1959\n",
      "3   Lagaan: Once Upon a Time in India    8.1  2001\n",
      "4                     Pather Panchali    8.2  1955\n",
      "..                                ...    ...   ...\n",
      "95                        Apur Sansar    8.4  1959\n",
      "96                        Kanchivaram    8.2  2008\n",
      "97                    Monsoon Wedding    7.3  2001\n",
      "98                              Black    8.1  2005\n",
      "99                            Deewaar      8  1975\n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the IMDb list\n",
    "url = 'https://www.imdb.com/list/ls056092300/'\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find all the movie items on the page\n",
    "movies = soup.find_all('div', class_='lister-item-content')\n",
    "\n",
    "# Lists to store movie data\n",
    "names = []\n",
    "ratings = []\n",
    "years = []\n",
    "\n",
    "# Iterate over each movie and extract data\n",
    "for movie in movies:\n",
    "    # Name of the movie\n",
    "    name = movie.find('a').text.strip()\n",
    "    names.append(name)\n",
    "    \n",
    "    # Rating of the movie\n",
    "    rating = movie.find('span', class_='ipl-rating-star__rating').text.strip()\n",
    "    ratings.append(rating)\n",
    "    \n",
    "    # Year of release of the movie\n",
    "    year = movie.find('span', class_='lister-item-year').text.strip('()')\n",
    "    years.append(year)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Name': names,\n",
    "    'Rating': ratings,\n",
    "    'Year': years\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fd40219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://peachmode.com/search?q=bags'\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find all products on the page\n",
    "products = soup.find_all('div', class_='product-item')\n",
    "\n",
    "for product in products:\n",
    "    # Extract product name\n",
    "    name = product.find('h2', class_='product-name').text.strip()\n",
    "    \n",
    "    # Extract product price\n",
    "    price = product.find('span', class_='price-new').text.strip()\n",
    "    \n",
    "    # Extract discount, if available\n",
    "    discount_elem = product.find('span', class_='discount')\n",
    "    discount = discount_elem.text.strip() if discount_elem else 'No discount available'\n",
    "    \n",
    "    print(f'Product Name: {name}\\nPrice: {price}\\nDiscount: {discount}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2477554",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_name_' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 66\u001b[0m\n\u001b[0;32m     62\u001b[0m         bowlers\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlayer\u001b[39m\u001b[38;5;124m'\u001b[39m: player_name, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTeam\u001b[39m\u001b[38;5;124m'\u001b[39m: team, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRating\u001b[39m\u001b[38;5;124m'\u001b[39m: rating})\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bowlers\n\u001b[1;32m---> 66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _name_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_main_\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTop 10 ODI Teams:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     68\u001b[0m     teams \u001b[38;5;241m=\u001b[39m scrape_odi_teams()\n",
      "\u001b[1;31mNameError\u001b[0m: name '_name_' is not defined"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_odi_teams():\n",
    "    url = \"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    teams = []\n",
    "    for team in soup.find_all('tr', class_='rankings-block__banner')[:1]:  # Top team\n",
    "        team_name = team.find('span', class_='u-hide-phablet').text.strip()\n",
    "        matches = team.find('td', class_='rankings-block__banner--matches').text.strip()\n",
    "        points = team.find('td', class_='rankings-block__banner--points').text.strip()\n",
    "        rating = team.find('td', class_='rankings-block__banner--rating').text.strip()\n",
    "        teams.append({'Team': team_name, 'Matches': matches, 'Points': points, 'Rating': rating})\n",
    "\n",
    "    for team in soup.find_all('tr', class_='table-body')[:9]:  # Remaining teams\n",
    "        team_name = team.find('span', class_='u-hide-phablet').text.strip()\n",
    "        matches = team.find_all('td')[2].text.strip()\n",
    "        points = team.find_all('td')[3].text.strip()\n",
    "        rating = team.find_all('td')[4].text.strip()\n",
    "        teams.append({'Team': team_name, 'Matches': matches, 'Points': points, 'Rating': rating})\n",
    "\n",
    "    return teams\n",
    "\n",
    "def scrape_odi_batsmen():\n",
    "    url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    batsmen = []\n",
    "    for player in soup.find_all('tr', class_='rankings-block__banner')[:1]:  # Top player\n",
    "        player_name = player.find('div', class_='rankings-block__banner--name').text.strip()\n",
    "        team = player.find('div', class_='rankings-block__banner--nationality').text.strip()\n",
    "        rating = player.find('div', class_='rankings-block__banner--rating').text.strip()\n",
    "        batsmen.append({'Player': player_name, 'Team': team, 'Rating': rating})\n",
    "\n",
    "    for player in soup.find_all('tr', class_='table-body')[:9]:  # Remaining players\n",
    "        player_name = player.find('td', class_='table-body__cell name').text.strip()\n",
    "        team = player.find('span', class_='table-body__logo-text').text.strip()\n",
    "        rating = player.find('td', class_='table-body__cell u-text-right rating').text.strip()\n",
    "        batsmen.append({'Player': player_name, 'Team': team, 'Rating': rating})\n",
    "\n",
    "    return batsmen\n",
    "\n",
    "def scrape_odi_bowlers():\n",
    "    url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    bowlers = []\n",
    "    for player in soup.find_all('tr', class_='rankings-block__banner')[:1]:  # Top player\n",
    "        player_name = player.find('div', class_='rankings-block__banner--name').text.strip()\n",
    "        team = player.find('div', class_='rankings-block__banner--nationality').text.strip()\n",
    "        rating = player.find('div', class_='rankings-block__banner--rating').text.strip()\n",
    "        bowlers.append({'Player': player_name, 'Team': team, 'Rating': rating})\n",
    "\n",
    "    for player in soup.find_all('tr', class_='table-body')[:9]:  # Remaining players\n",
    "        player_name = player.find('td', class_='table-body__cell name').text.strip()\n",
    "        team = player.find('span', class_='table-body__logo-text').text.strip()\n",
    "        rating = player.find('td', class_='table-body__cell u-text-right rating').text.strip()\n",
    "        bowlers.append({'Player': player_name, 'Team': team, 'Rating': rating})\n",
    "\n",
    "    return bowlers\n",
    "\n",
    "if _name_ == \"_main_\":\n",
    "    print(\"Top 10 ODI Teams:\")\n",
    "    teams = scrape_odi_teams()\n",
    "    for team in teams:\n",
    "        print(team)\n",
    "    \n",
    "    print(\"\\nTop 10 ODI Batsmen:\")\n",
    "    batsmen = scrape_odi_batsmen()\n",
    "    for player in batsmen:\n",
    "        print(player)\n",
    "    \n",
    "    print(\"\\nTop 10 ODI Bowlers:\")\n",
    "    bowlers = scrape_odi_bowlers()\n",
    "    for player in bowlers:\n",
    "        print(player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d432435",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.patreon.com/coreyms\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "posts = soup.find_all('div', class_='postCard_post__1zRsm')\n",
    "\n",
    "for post in posts:\n",
    "    heading = post.find('h3', class_='postCard_title__1YUQC').text.strip()\n",
    "    date = post.find('time', class_='postCard_date__1VwY9').text.strip()\n",
    "    content = post.find('div', class_='postCard_content__1Zdo9').text.strip()\n",
    "    \n",
    "    # Find the link for the youtube video\n",
    "    video_link = post.find('a', class_='postCard_videoLink__11HkZ')['href']\n",
    "    \n",
    "    # Scrape likes for the video from the video page\n",
    "    video_response = requests.get(video_link)\n",
    "    video_soup = BeautifulSoup(video_response.text, 'html.parser')\n",
    "    likes = video_soup.find('button', class_='like-button-renderer-like-button-unclicked').text.strip()\n",
    "    \n",
    "    print(\"Heading:\", heading)\n",
    "    print(\"Date:\", date)\n",
    "    print(\"Content:\", content)\n",
    "    print(\"Likes for the video:\", likes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7763dea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping house details for Indira-nagar:\n",
      "Scraping house details for Jayanagar:\n",
      "Scraping house details for Rajaji-nagar:\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_house_details(locality):\n",
    "    url = f\"https://www.nobroker.in/property/sale/{locality}/?searchParam=W3sibGF0IjoxMy4wMDM2MTQ0LCJsb24iOjc3LjgxNDMzMjgsInBsYWNlSWQiOiJDaElKNTNhZUNWcURBemI2Rl9INXRyRWxXQ1FNIiwicGxhY2VOYW1lIjoiSW5kaXJhIE5hZ2FyIn1d&radius=2.0&type=BHK4\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    houses = soup.find_all('div', class_='nb__2JHKO')\n",
    "\n",
    "    for house in houses:\n",
    "        title = house.find('h2', class_='heading-6').text.strip()\n",
    "        location = house.find('div', class_='nb__35Ol7').text.strip()\n",
    "        area = house.find('div', class_='nb__3oNyC').text.strip()\n",
    "        emi = house.find('div', class_='font-semi-bold heading-6', text='EMI').find_next_sibling('div').text.strip()\n",
    "        price = house.find('div', class_='font-semi-bold heading-6').text.strip()\n",
    "\n",
    "        print(\"Title:\", title)\n",
    "        print(\"Location:\", location)\n",
    "        print(\"Area:\", area)\n",
    "        print(\"EMI:\", emi)\n",
    "        print(\"Price:\", price)\n",
    "        print(\"--------------\")\n",
    "\n",
    "localities = [\"indira-nagar\", \"jayanagar\", \"rajaji-nagar\"]\n",
    "\n",
    "for locality in localities:\n",
    "    print(f\"Scraping house details for {locality.capitalize()}:\")\n",
    "    scrape_house_details(locality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89fa4cd4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m products \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproductCardBox\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m product \u001b[38;5;129;01min\u001b[39;00m products[:\u001b[38;5;241m10\u001b[39m]:  \u001b[38;5;66;03m# Scraping first 10 products\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     name \u001b[38;5;241m=\u001b[39m product\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproduct-title\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     12\u001b[0m     price \u001b[38;5;241m=\u001b[39m product\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspan\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moriginal-price\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     13\u001b[0m     image_url \u001b[38;5;241m=\u001b[39m product\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msrc\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.bewakoof.com/bestseller?sort=popular\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "products = soup.find_all('div', class_='productCardBox')\n",
    "\n",
    "for product in products[:10]:  # Scraping first 10 products\n",
    "    name = product.find('div', class_='product-title').text.strip()\n",
    "    price = product.find('span', class_='original-price').text.strip()\n",
    "    image_url = product.find('img')['src']\n",
    "\n",
    "    print(\"Product Name:\", name)\n",
    "    print(\"Price:\", price)\n",
    "    print(\"Image URL:\", image_url)\n",
    "    print(\"----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173f9ce7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
