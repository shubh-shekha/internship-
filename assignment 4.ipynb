{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cfe19b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'python' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m python\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbs4\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup\n",
      "\u001b[1;31mNameError\u001b[0m: name 'python' is not defined"
     ]
    }
   ],
   "source": [
    "python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the table containing the details of most viewed videos\n",
    "table = soup.find(\"table\", {\"class\": \"wikitable\"})\n",
    "\n",
    "# Initialize lists to store the details\n",
    "rank_list = []\n",
    "name_list = []\n",
    "artist_list = []\n",
    "upload_date_list = []\n",
    "views_list = []\n",
    "\n",
    "# Iterate through rows of the table\n",
    "for row in table.find_all(\"tr\")[1:]:\n",
    "    columns = row.find_all(\"td\")\n",
    "    rank = columns[0].text.strip()\n",
    "    name = columns[1].text.strip()\n",
    "    artist = columns[2].text.strip()\n",
    "    upload_date = columns[3].text.strip()\n",
    "    views = columns[4].text.strip()\n",
    "\n",
    "    # Append details to respective lists\n",
    "    rank_list.append(rank)\n",
    "    name_list.append(name)\n",
    "    artist_list.append(artist)\n",
    "    upload_date_list.append(upload_date)\n",
    "    views_list.append(views)\n",
    "\n",
    "# Print or further process the lists as needed\n",
    "for i in range(len(rank_list)):\n",
    "    print(f\"Rank: {rank_list[i]}\")\n",
    "    print(f\"Name: {name_list[i]}\")\n",
    "    print(f\"Artist: {artist_list[i]}\")\n",
    "    print(f\"Upload Date: {upload_date_list[i]}\")\n",
    "    print(f\"Views: {views_list[i]}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d90640d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2527209388.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    [8:30 PM, 4/7/2024] shubh Shekhar: import requests\u001b[0m\n\u001b[1;37m      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "[8:30 PM, 4/7/2024] shubh Shekhar: import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Fetch the BCCI website home page\n",
    "url = \"https://www.bcci.tv/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the link to the international fixtures page\n",
    "fixtures_link = soup.find(\"a\", text=\"International Fixtures\")['href']\n",
    "fixtures_url = f\"https://www.bcci.tv{fixtures_link}\"\n",
    "\n",
    "# Fetch the international fixtures page\n",
    "fixtures_response = requests.get(fixtures_url)\n",
    "fixtures_soup = BeautifulSoup(fixtures_response.content, \"html.parser\")\n",
    "\n",
    "# Find the container containing the fixtures\n",
    "fixtures_container = fixtures_soup.find(\"div\", class_=\"js-list\")\n",
    "\n",
    "# Initialize lists to store the details\n",
    "series_list = []\n",
    "place_list = []\n",
    "date_list = []\n",
    "time_list = []\n",
    "\n",
    "# Extract details of each fixture\n",
    "fixtures = fixtures_container.find_all(\"div\", class_=\"fixture__info\")\n",
    "for fixture in fixtures:\n",
    "    series = fixture.find(\"span\", class_=\"u-unskewed-text\").text.strip()\n",
    "    place = fixture.find(\"p\", class_=\"fixture__additional-info\").text.strip()\n",
    "    date = fixture.find(\"div\", class_=\"fixture_datetime desktop-only\").find(\"span\", class=\"fixture__date\").text.strip()\n",
    "    time = fixture.find(\"div\", class_=\"fixture_datetime desktop-only\").find(\"span\", class=\"fixture__time\").text.strip()\n",
    "\n",
    "    # Append details to respective lists\n",
    "    series_list.append(series)\n",
    "    place_list.append(place)\n",
    "    date_list.append(date)\n",
    "    time_list.append(time)\n",
    "\n",
    "# Print or further process the lists as needed\n",
    "for i in range(len(series_list)):\n",
    "    print(f\"Series: {series_list[i]}\")\n",
    "    print(f\"Place: {place_list[i]}\")\n",
    "    print(f\"Date: {date_list[i]}\")\n",
    "    print(f\"Time: {time_list[i]}\")\n",
    "    print(\"-\" * 50)\n",
    "[8:32 PM, 4/7/2024] shubh Shekhar: import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Fetch the statisticstimes home page\n",
    "url = \"http://statisticstimes.com/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the link to the economy page\n",
    "economy_link = soup.find(\"div\", class_=\"dropdown-content\").find(\"a\", text=\"Economy\")['href']\n",
    "economy_url = f\"http://statisticstimes.com/{economy_link}\"\n",
    "\n",
    "# Fetch the economy page\n",
    "economy_response = requests.get(economy_url)\n",
    "economy_soup = BeautifulSoup(economy_response.content, \"html.parser\")\n",
    "\n",
    "# Find the link to the State-wise GDP page\n",
    "state_gdp_link = economy_soup.find(\"a\", text=\"GDP of Indian states\").get('href')\n",
    "state_gdp_url = f\"http://statisticstimes.com/{state_gdp_link}\"\n",
    "\n",
    "# Fetch the State-wise GDP page\n",
    "state_gdp_response = requests.get(state_gdp_url)\n",
    "state_gdp_soup = BeautifulSoup(state_gdp_response.content, \"html.parser\")\n",
    "\n",
    "# Find the table containing the State-wise GDP details\n",
    "table = state_gdp_soup.find(\"table\", {\"id\": \"table_id\"})\n",
    "\n",
    "# Initialize lists to store the details\n",
    "rank_list = []\n",
    "state_list = []\n",
    "gsdp_18_19_list = []\n",
    "gsdp_19_20_list = []\n",
    "share_18_19_list = []\n",
    "gdp_billion_list = []\n",
    "\n",
    "# Extract details of each state\n",
    "rows = table.find_all(\"tr\")[1:]\n",
    "for row in rows:\n",
    "    columns = row.find_all(\"td\")\n",
    "    rank = columns[0].text.strip()\n",
    "    state = columns[1].text.strip()\n",
    "    gsdp_18_19 = columns[2].text.strip()\n",
    "    gsdp_19_20 = columns[3].text.strip()\n",
    "    share_18_19 = columns[4].text.strip()\n",
    "    gdp_billion = columns[5].text.strip()\n",
    "\n",
    "    # Append details to respective lists\n",
    "    rank_list.append(rank)\n",
    "    state_list.append(state)\n",
    "    gsdp_18_19_list.append(gsdp_18_19)\n",
    "    gsdp_19_20_list.append(gsdp_19_20)\n",
    "    share_18_19_list.append(share_18_19)\n",
    "    gdp_billion_list.append(gdp_billion)\n",
    "\n",
    "# Print or further process the lists as needed\n",
    "for i in range(len(rank_list)):\n",
    "    print(f\"Rank: {rank_list[i]}\")\n",
    "    print(f\"State: {state_list[i]}\")\n",
    "    print(f\"GSDP(18-19) - Current Prices: {gsdp_18_19_list[i]}\")\n",
    "    print(f\"GSDP(19-20) - Current Prices: {gsdp_19_20_list[i]}\")\n",
    "    print(f\"Share(18-19): {share_18_19_list[i]}\")\n",
    "    print(f\"GDP($ billion): {gdp_billion_list[i]}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcf67ad6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Extract details of each trending repository\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m repo \u001b[38;5;129;01min\u001b[39;00m repo_containers:\n\u001b[1;32m---> 28\u001b[0m     title \u001b[38;5;241m=\u001b[39m repo\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh1\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     29\u001b[0m     description \u001b[38;5;241m=\u001b[39m repo\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol-9\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     30\u001b[0m     contributors_count \u001b[38;5;241m=\u001b[39m repo\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLink--muted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Fetch the GitHub home page\n",
    "url = \"https://github.com/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the link to the trending page\n",
    "trending_link = soup.find(\"a\", href=\"/trending\").get('href')\n",
    "trending_url = f\"https://github.com{trending_link}\"\n",
    "\n",
    "# Fetch the trending page\n",
    "trending_response = requests.get(trending_url)\n",
    "trending_soup = BeautifulSoup(trending_response.content, \"html.parser\")\n",
    "\n",
    "# Find the containers containing the trending repositories\n",
    "repo_containers = trending_soup.find_all(\"article\", class_=\"Box-row\")\n",
    "\n",
    "# Initialize lists to store the details\n",
    "repository_titles = []\n",
    "repository_descriptions = []\n",
    "contributors_counts = []\n",
    "languages_used = []\n",
    "\n",
    "# Extract details of each trending repository\n",
    "for repo in repo_containers:\n",
    "    title = repo.find(\"h1\", class_=\"h3\").text.strip()\n",
    "    description = repo.find(\"p\", class_=\"col-9\").text.strip()\n",
    "    contributors_count = repo.find(\"a\", class_=\"Link--muted\").text.strip()\n",
    "    language_used = repo.find(\"span\", itemprop=\"programmingLanguage\").text.strip() if repo.find(\"span\", itemprop=\"programmingLanguage\") else \"Not specified\"\n",
    "\n",
    "    # Append details to respective lists\n",
    "    repository_titles.append(title)\n",
    "    repository_descriptions.append(description)\n",
    "    contributors_counts.append(contributors_count)\n",
    "    languages_used.append(language_used)\n",
    "\n",
    "# Print or further process the lists as needed\n",
    "for i in range(len(repository_titles)):\n",
    "    print(f\"Repository Title: {repository_titles[i]}\")\n",
    "    print(f\"Repository Description: {repository_descriptions[i]}\")\n",
    "    print(f\"Contributors Count: {contributors_counts[i]}\")\n",
    "    print(f\"Language Used: {languages_used[i]}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab92f010",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_9548\\3227772263.py:10: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  charts_link = soup.find(\"a\", text=\"Charts\").get('href')\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Find the link to the Hot 100 page\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m charts_link \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m, text\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCharts\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m charts_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.billboard.com\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcharts_link\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Fetch the charts page\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Fetch the Billboard home page\n",
    "url = \"https://www.billboard.com/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the link to the Hot 100 page\n",
    "charts_link = soup.find(\"a\", text=\"Charts\").get('href')\n",
    "charts_url = f\"https://www.billboard.com{charts_link}\"\n",
    "\n",
    "# Fetch the charts page\n",
    "charts_response = requests.get(charts_url)\n",
    "charts_soup = BeautifulSoup(charts_response.content, \"html.parser\")\n",
    "\n",
    "# Find the link to the Hot 100 page\n",
    "hot_100_link = charts_soup.find(\"a\", text=\"Hot 100\").get('href')\n",
    "hot_100_url = f\"https://www.billboard.com{hot_100_link}\"\n",
    "\n",
    "# Fetch the Hot 100 page\n",
    "hot_100_response = requests.get(hot_100_url)\n",
    "hot_100_soup = BeautifulSoup(hot_100_response.content, \"html.parser\")\n",
    "\n",
    "# Find the container containing the top 100 songs details\n",
    "song_containers = hot_100_soup.find_all(\"div\", class_=\"chart-list-item\")\n",
    "\n",
    "# Initialize lists to store the details\n",
    "song_names = []\n",
    "artist_names = []\n",
    "last_week_ranks = []\n",
    "peak_ranks = []\n",
    "weeks_on_board = []\n",
    "\n",
    "# Extract details of each song\n",
    "for song in song_containers:\n",
    "    song_name = song.find(\"span\", class_=\"chart-element_information_song\").text.strip()\n",
    "    artist_name = song.find(\"span\", class_=\"chart-element_information_artist\").text.strip()\n",
    "    last_week_rank = song.find(\"div\", class_=\"chart-element__meta text--center color--secondary text--last\").text.strip()\n",
    "    peak_rank = song.find(\"div\", class_=\"chart-element__meta text--center color--secondary text--peak\").text.strip()\n",
    "    weeks_on_board = song.find(\"div\", class_=\"chart-element__meta text--center color--secondary text--week\").text.strip()\n",
    "\n",
    "    # Append details to respective lists\n",
    "    song_names.append(song_name)\n",
    "    artist_names.append(artist_name)\n",
    "    last_week_ranks.append(last_week_rank)\n",
    "    peak_ranks.append(peak_rank)\n",
    "    weeks_on_board.append(weeks_on_board)\n",
    "\n",
    "# Print or further process the lists as needed\n",
    "for i in range(len(song_names)):\n",
    "    print(f\"Song Name: {song_names[i]}\")\n",
    "    print(f\"Artist Name: {artist_names[i]}\")\n",
    "    print(f\"Last Week Rank: {last_week_ranks[i]}\")\n",
    "    print(f\"Peak Rank: {peak_ranks[i]}\")\n",
    "    print(f\"Weeks on Board: {weeks_on_board[i]}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "186fafc8",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m columns \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtd\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     23\u001b[0m book_name \u001b[38;5;241m=\u001b[39m columns[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m---> 24\u001b[0m author_name \u001b[38;5;241m=\u001b[39m columns[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     25\u001b[0m volumes \u001b[38;5;241m=\u001b[39m columns[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     26\u001b[0m publisher \u001b[38;5;241m=\u001b[39m columns[\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Fetch the page containing the highest-selling novels\n",
    "url = \"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the table containing the details\n",
    "table = soup.find(\"table\")\n",
    "\n",
    "# Initialize lists to store the details\n",
    "book_names = []\n",
    "author_names = []\n",
    "volumes_sold = []\n",
    "publishers = []\n",
    "genres = []\n",
    "\n",
    "# Extract details of each novel\n",
    "rows = table.find_all(\"tr\")[1:]  # skipping the header row\n",
    "for row in rows:\n",
    "    columns = row.find_all(\"td\")\n",
    "    book_name = columns[0].text.strip()\n",
    "    author_name = columns[1].text.strip()\n",
    "    volumes = columns[2].text.strip()\n",
    "    publisher = columns[3].text.strip()\n",
    "    genre = columns[4].text.strip()\n",
    "\n",
    "    # Append details to respective lists\n",
    "    book_names.append(book_name)\n",
    "    author_names.append(author_name)\n",
    "    volumes_sold.append(volumes)\n",
    "    publishers.append(publisher)\n",
    "    genres.append(genre)\n",
    "\n",
    "# Print or further process the lists as needed\n",
    "for i in range(len(book_names)):\n",
    "    print(f\"Book Name: {book_names[i]}\")\n",
    "    print(f\"Author Name: {author_names[i]}\")\n",
    "    print(f\"Volumes Sold: {volumes_sold[i]}\")\n",
    "    print(f\"Publisher: {publishers[i]}\")\n",
    "    print(f\"Genre: {genres[i]}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f022da9c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (701373909.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[6], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    [8:37 PM, 4/7/2024] shubh Shekhar: import requests\u001b[0m\n\u001b[1;37m      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "[8:37 PM, 4/7/2024] shubh Shekhar: import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Fetch the IMDb page containing the most watched TV series\n",
    "url = \"https://www.imdb.com/list/ls095964455/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the container containing the TV series details\n",
    "series_containers = soup.find_all(\"div\", class_=\"lister-item-content\")\n",
    "\n",
    "# Initialize lists to store the details\n",
    "names = []\n",
    "year_spans = []\n",
    "genres = []\n",
    "runtimes = []\n",
    "ratings = []\n",
    "votes = []\n",
    "\n",
    "# Extract details of each TV series\n",
    "for series in series_containers:\n",
    "    name = series.find(\"h3\", class_=\"lister-item-header\").a.text.strip()\n",
    "    year_span = series.find(\"span\", class_=\"lister-item-year\").text.strip()\n",
    "    genre = series.find(\"span\", class_=\"genre\").text.strip()\n",
    "    runtime = series.find(\"span\", class_=\"runtime\").text.strip()\n",
    "    rating = series.find(\"span\", class_=\"ipl-rating-star__rating\").text.strip()\n",
    "    vote = series.find(\"span\", {\"name\": \"nv\"})[\"data-value\"]\n",
    "\n",
    "    # Append details to respective lists\n",
    "    names.append(name)\n",
    "    year_spans.append(year_span)\n",
    "    genres.append(genre)\n",
    "    runtimes.append(runtime)\n",
    "    ratings.append(rating)\n",
    "    votes.append(vote)\n",
    "\n",
    "# Print or further process the lists as needed\n",
    "for i in range(len(names)):\n",
    "    print(f\"Name: {names[i]}\")\n",
    "    print(f\"Year Span: {year_spans[i]}\")\n",
    "    print(f\"Genre: {genres[i]}\")\n",
    "    print(f\"Run Time: {runtimes[i]}\")\n",
    "    print(f\"Ratings: {ratings[i]}\")\n",
    "    print(f\"Votes: {votes[i]}\")\n",
    "    print(\"-\" * 50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ca44322",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_9548\\152663845.py:10: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  all_dataset_link = soup.find(\"a\", text=\"View ALL Data Sets\").get('href')\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Find the link to the \"Show All Dataset\" page\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m all_dataset_link \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m, text\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mView ALL Data Sets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m all_dataset_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://archive.ics.uci.edu/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mall_dataset_link\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Fetch the \"Show All Dataset\" page\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Fetch the UCI Machine Learning Repository home page\n",
    "url = \"https://archive.ics.uci.edu/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the link to the \"Show All Dataset\" page\n",
    "all_dataset_link = soup.find(\"a\", text=\"View ALL Data Sets\").get('href')\n",
    "all_dataset_url = f\"https://archive.ics.uci.edu/{all_dataset_link}\"\n",
    "\n",
    "# Fetch the \"Show All Dataset\" page\n",
    "all_dataset_response = requests.get(all_dataset_url)\n",
    "all_dataset_soup = BeautifulSoup(all_dataset_response.content, \"html.parser\")\n",
    "\n",
    "# Find the table containing the dataset details\n",
    "table = all_dataset_soup.find(\"table\", {\"border\": \"1\"})\n",
    "\n",
    "# Initialize lists to store the details\n",
    "dataset_names = []\n",
    "data_types = []\n",
    "tasks = []\n",
    "attribute_types = []\n",
    "instances_counts = []\n",
    "attributes_counts = []\n",
    "years = []\n",
    "\n",
    "# Extract details of each dataset\n",
    "rows = table.find_all(\"tr\")[1:]  # skipping the header row\n",
    "for row in rows:\n",
    "    columns = row.find_all(\"td\")\n",
    "    dataset_name = columns[0].text.strip()\n",
    "    data_type = columns[1].text.strip()\n",
    "    task = columns[2].text.strip()\n",
    "    attribute_type = columns[3].text.strip()\n",
    "    instances_count = columns[4].text.strip()\n",
    "    attributes_count = columns[5].text.strip()\n",
    "    year = columns[6].text.strip()\n",
    "\n",
    "    # Append details to respective lists\n",
    "    dataset_names.append(dataset_name)\n",
    "    data_types.append(data_type)\n",
    "    tasks.append(task)\n",
    "    attribute_types.append(attribute_type)\n",
    "    instances_counts.append(instances_count)\n",
    "    attributes_counts.append(attributes_count)\n",
    "    years.append(year)\n",
    "\n",
    "# Print or further process the lists as needed\n",
    "for i in range(len(dataset_names)):\n",
    "    print(f\"Dataset Name: {dataset_names[i]}\")\n",
    "    print(f\"Data Type: {data_types[i]}\")\n",
    "    print(f\"Task: {tasks[i]}\")\n",
    "    print(f\"Attribute Type: {attribute_types[i]}\")\n",
    "    print(f\"No of Instances: {instances_counts[i]}\")\n",
    "    print(f\"No of Attributes: {attributes_counts[i]}\")\n",
    "    print(f\"Year: {years[i]}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d839533",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
